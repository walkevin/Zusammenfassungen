\section{Flexible regression and classification methods}
\begin{application}
 \begin{itemize}
  \item Fully nonparametric models are very expensive due to the curse of dimensionality.
  \item All of the following models are of nonparametric nature.
  \item They all have some structural assumptions to mitigate the curse of dimensionality.
 \end{itemize}
\end{application}
\subsection{Additve models}
\begin{theory}
\begin{itemize}
 \item Let a regression function $\mathbb{E}[Y|X = x]$ or a binary classification problem $\log(\pi(x)/(1-\pi(x))$ be denoted by 
 \begin{equation*}
  g(\cdot):\mathbb{R}^p\rightarrow\mathbb{R}
 \end{equation*}
 \item The model is
 \begin{gather*}
  g_{\text{add}}(x) = \mu + \sum_{j=1}^p g_j(x_j), \quad\mu\in\mathbb{R}\\
  g_j(\cdot): \mathbb{R}\rightarrow\mathbb{R},\quad\mathbb{E}[g_j(X_j)] = 0\quad (j=1,\cdots,p)
 \end{gather*}
 \item No curse of dimensionality, because the individual $g_j$ only depend on a 1-dimensional variable.
\end{itemize}
\end{theory}
\begin{application}
\begin{itemize}
 \item[\leftthumbsup] Additve model doesn't suffer from curse of dimensionality.
 \item[\leftthumbsdown] No interaction terms such as $g_{j,k}(x_j, x_k)$ 
\end{itemize}
\end{application}

\begin{code}
 library(mgcv) # for gam
 library(sfsmisc) # for wrapFormula
 data(ozone, package="gss")
 form <- as.formula("upo3 ~.")
 
 # Do the GAM fit
 gamForm <- wrapFormula(formula = form, data = ozone, wrapString = "s(*)") #s is part of mgcv
 fit <- gam(gamForm, data = ozone) 
 summary(fit)
\end{code}

\subsection{Multivariate adaptive regression splines (MARS)}
\begin{application}
\begin{itemize}
 \item[\leftthumbsup] Often useful for high-dimensional problems with many predictor variables.
 \item MARS with interaction degree = 1 yields an additive model.
\end{itemize}
\end{application}

\begin{theory}
 The model is
 \begin{equation*}
  g(x) = \mu + \sum_{m=1}^M \beta_m h_m(x)
 \end{equation*}
 where $\beta_m \in \mathbb{R}$ and each $h_m(x)$ is a function (or a product of functions) from the basis $\mathcal{B}$.
 The basis $\mathcal{B}$ consists of so called \textit{hinge functions}.
 \begin{equation*}
  \mathcal{B} = \{x_j - d)_+, (d - x_j)_+;\quad d \in \{x_{1,j}, x_{2,j},\cdots, x_{n,j}, j \in \{1,\cdots,p\}
 \end{equation*}
 A hinge function is defined as
 \begin{equation*}
  (x_j - d)_+ = \begin{cases}
                 x_j - d & \text{if} x_j > d\\
                 0       & \text{otherwise}
                \end{cases}
 \end{equation*}
 A sample output from a MARS fitting could be
\parbox{\linewidth}{
    \texttt{
    \begin{tabular}{ll}
                            & coefficients \\
    (Intercept)                & 2.78936250 \\
    h(47-hmdt)                 &-0.01321978 \\ 
    h(52-sbtp)                 &-0.01779088 \\
    h(200-vsty)                & 0.00196268 \\ 
    h(wdsp-7) * h(200-vsty)    &-0.00152861 \\
    h(44-hmdt) * h(52-sbtp)    & 0.00179851    
    \end{tabular}
    }
}
The variables $x_1, \cdots, x_4$ are \textit{hmdt, sbtp, vsty} and \textit{wdsp}. The chosen hinge functions $h_m$ are shown in the left column. The coefficients $\beta_m$ are shown in the right column.
%TODO: Some reference to the algorithm.
\end{theory}

\begin{code}
 ozone <- read.table("...")
 library(earth) # earth is open-source
 fit <- earth(upo3 ~ ., data = ozone, degree = 2)
 summary(fit)
 
 # Plot interactions
 plotmo(fit, degree2 = FALSE)
 plotmo(fit, degree1 = FALSE)
\end{code}


\subsection{Projection pursuit regression (PPR)}
\begin{theory}
The model is
 \begin{gather*}
  g_{\text{PPR}}(x) = \mu + \sum_{k=1}^q \beta_k f_k \left( \sum_{j=1}^p \alpha_{jk}x_j \right), \quad\text{where}\\
  \sum_{j=1}^p \alpha_{jk}^2 = 1, \mathbb{E}[f_k \left( \sum_{j=1}^p \alpha_{jk}X_j \right)]=0,\quad\text{for all $k$}
 \end{gather*}
 The nonparametric functions $f_k(\cdot):\mathbb{R}\rightarrow\mathbb{R}$ are the so called \textit{ridge functions}.
 A sample output from a PPR fitting could be
 \parbox{\linewidth}{
  \texttt{
   \begin{tabular}{lll}
    \multicolumn{3}{l}{Projection direction vectors:}\\
         & term 1   & term 2\\
    vdht & 0.00021  & -0.07819\\
    wdsp & -0.09305 & -0.95991\\
    sbtp & 0.91062  & 0.23228\\
    ibht & -0.00086 & 0.00262 
   \end{tabular}\\
   \begin{tabular}{ll}
    \multicolumn{2}{l}{Coefficients of ridge terms:}\\
    term 1  & term 2\\
    0.53914 & 0.10202
   \end{tabular}
  }
 }
 \begin{itemize}
  \item PPR was carried out with 2 terms ($k=2$).
  \item The projection direction vectors are the $\alpha_{jk}$. \textit{ibht} has a negligible overall effect, \textit{wdsp} dominates \textit{term 2}.
  \item Coefficients of ridge terms are the $\beta_k$.
 \end{itemize}

\end{theory}
\begin{application}
 \begin{itemize}
  \item In general, it is good if the ridge functions look smoothely.
  \item A more objective measure is of course a CV score comparison.
 \end{itemize}
\end{application}

\begin{code}
 ozone <- read.table("...")
 # Important options are:
 # sm.method = {"supsmu", "spline", "gcvspline"}
 # nterms
 # df for spline
 # bass, span for supsmu
 # gcvpen for gcvspline
 fit <- ppr(upo3 ~ ., data = ozone, nterms=4)
 par(mfrow=c(2,2))
 plot(fit)
 predict(fit, xnew)
\end{code}

\subsection{Classification and Regression Trees (CART)}
\begin{theory}
 The model function is piecewise constant.
 \begin{equation*}
  g_{\text{tree}}(x) = \sum_{r=1}^M \beta_r \mathbf{1}_{[x\in\mathcal{R}_r]}
 \end{equation*}
 where $\mathcal{P} = \{\mathcal{R}_1, \dotsc, \mathcal{R}_M\}$ is a partition of $\mathbb{R}^p$. $p$ is the number of explanatory variables.
 If the partition was given, $\beta_r$ would be estimated as
 \begin{equation*}
  \hat{\beta}_r = \sum_{i=1}^n Y_i \mathbf{1}_{[x_i\in\mathcal{R}_r]} / \mathbf{1}_{[x_i\in\mathcal{R}_r]}
 \end{equation*}
 which is the mean of the responses of the explanatory variables "living" in region $\mathcal{R}_r$.
 %TODO: some reference on the algorithm
 %TODO:\paragraph{Tree structure}
 \paragraph{Pruning and model selection}
 To decide on the height of the tree, an error for each tree is determined by cross-validation.
 The smallest tree is taken, whose error is within the standard error of the minimal error.
 \end{theory}

\begin{code}
 data <- read.table("...", header = TRUE)
 
 # Get full tree
 library(rpart)
 tree <- rpart(Y ~ ., data = data, control = rpart.control(cp = 0.0, minsplit = 30))
 
 # Plot/Print complexity parameter
 plotcp(tree)
 printcp(tree)
 
 # Prune tree
 ## choose optimal cp according to 1-std-error rule:
 min.ind <- which.min(tree\$cptable[,"xerror"])
 min.lim <- tree\$cptable[min.ind, "xerror"] + tree\$cptable[min.ind, "xstd"]
 cp.opt <- tree\$cptable[(tree\$cptable[,"xerror"] < min.lim),"CP"][1]
 
 ## Pruning
 tree.pruned <- prune.rpart(tree, cp = cp.opt)
 
 # Compute misclassification error
 # Test = Training data
 err1 <- mean(residuals(tree))
 # Test != Training data
 err2 <- mean(sapply(newdata\$Y - predict(tree.pruned, data = newdata, type="vector"), function(x) x*x))
 
 # Plotting tree
 library(rpart.plot)
 prp(tree, extra=1, type=1, 
   box.col=c('pink', 'palegreen3', 
             'lightsteelblue 2','lightgoldenrod 1')[tree\$frame\$yval])

 # Alternative plotting
 library(maptree)
 draw.tree(tree)
\end{code}

\subsection{Variable Selection, Regularization, Ridging and the Lasso}
\begin{application}
 \begin{itemize}
  \item For normal linear regression, the number of observations $n$ must be larger than the number of explanatory variables $p$.
  \item Ridging and Lasso can be used if $p \ll n$
  \item This situation can also arise, if several explanatory variables are highly correlated.
  \item Lasso is computationally more costly, but more interesting, because some $\beta_j$ will be zero for large enough $\lambda$
  \item Ridge regression is computationally easier, and $\beta_j$ will go to zero as $\lambda$ increases, but not be exactly zero.
  \item The larger $\lambda$, the higher is the bias for $\boldsymbol{\beta}$, but the lower its variance.
 \end{itemize}
\end{application}

\begin{theory}
 \paragraph{Ridge regression} is defined as
 \begin{equation*}
  \tilde{\boldsymbol{\beta}}(s) = \argmin_{\|\boldsymbol{\beta}\|^2 \leq s} \|\mathbf{Y} - X \boldsymbol{\beta}\|^2
 \end{equation*}
 This can be expressed with the Lagrange multiplier $\lambda$
 \begin{equation*}
  \hat{\boldsymbol{\beta}}^{\ast}(\lambda) = \argmin_{\boldsymbol{\beta}} \left( \|\mathbf{Y} - X\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|^2 \right)
 \end{equation*}
 This minimization problem is equivalent to the normal equations
 \begin{equation*}
  (X^TX+\lambda I) \hat{\boldsymbol{\beta}}^{\ast} = X^T\mathbf{Y}
 \end{equation*}

 \paragraph{Lasso} is defined as
 \begin{align*}
  \hat{\boldsymbol{\beta}}^{\ast}(\lambda) &= \argmin_{\boldsymbol{\beta}} \left( \|\mathbf{Y} - X\boldsymbol{\beta}\|^2 + \lambda \sum_{j=1}^p |\beta_j| \right)\\
                                           &= \argmin_{\boldsymbol{\beta}} \left( \|\mathbf{Y} - X\boldsymbol{\beta}\|^2 + \lambda \|\boldsymbol{\beta}\|_1 \right)
 \end{align*} 
 This is solved by quadratic programming.
 \paragraph{Optimal $\lambda$} The optimal $\lambda$ can be found e.g. by GCV, k-fold CV or by plotting $\boldsymbol{\beta}$ against $\lambda$ (ridge trace).
\end{theory}

\begin{code}
 dat <- read.table("...", header = TRUE);
 mm <- model.matrix(someFormula, data = dat)
 require(glmnet)
 ridge <- glmnet(x=mm, y=dat\$y, alpha=0)
 lasso <- glmnet(x=mm, y=dat\$y, alpha=1)
 
 # plot parameters (see plot.glmnet for help)
 plot(ridge, xvar="lambda")
 plot(lasso, xvar="lambda")
 
 # 10-fold CV to get optimal lambda
 set.seed(1)
 cv.eln <- cv.glmnet(mm, dat\$y, alpha=0.5, nfolds=10)
 ## Look up minimal lambda and 1SE lambda, resp.
 lmin <- cv.eln\$lambda.min
 l1se <- cv.eln\$lambda.1se
 ## Look up corresponding CV errors
 cvmin <- cv.eln\$cvm[match(lmin, cv.eln\$lambda)]
 cv1se <- cv.eln\$cvm[match(l1se, cv.eln\$lambda)]
 
\end{code}
