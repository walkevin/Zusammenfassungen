\section{Nonparametric regression}
\subsection{Nadaraya-Watson kernel estimator}
\begin{theory}
 Definition of estimator:
 \begin{equation}
 \hat{m}(x) = \frac{\sum_{i=1}^n K((x-x_i)/h) Y_i}{\sum_{i=1}^n K((x-x_i)/h)}
 \end{equation}
 
 Alternative definition:
 \begin{equation}
 \hat{m}(x) = \argmin_{m_x \in \mathbb{R}} \sum_{i=1}^n K \left( \frac{x-x_i}{h} \right) (Y_i - m_x)^2
 \end{equation}
\end{theory}

\begin{application}
%TODO: How to assess that linear regression is bad (e.g. QQPlot etc. with code)
\begin{itemize}
 \item[\leftthumbsup] Nonparametric regression should be used if there is no linear relationship between response (y) and predictor (x)
 \item[\leftthumbsdown] The estimation accuracy is lower than for linear regression.
\end{itemize}
%Source: Script section 3.1
\end{application}

\begin{code}
 # Input values
 rangeX <- range(x) # Range of points in output
 xpts <- x # Points at which to evaluate the fit
 
 # Method
 result <- ksmooth(x, y, kernel = "normal", bandwidth = 0.5, range.x = rangeX, x.points = xpts)
 
 # Output values
 m <- result\$y # Fitted values
 mx <- result\$x # Points where smoothed fit is evaluated. (See x.points)
\end{code}

\subsection{Local polynomial regression estimator}
\begin{theory}
 Definition of estimator:
 \begin{subequations}
  \begin{multline}
   \hat{\boldsymbol{\beta}}(x) = \argmin_{\boldsymbol{\beta} \in \mathbb{R}^p} \sum_{i=1}^n K \left( \frac{x-x_i}{h} \right) \\
   \left(Y_i - \beta_0 - \beta_1(x_i - x) - \cdots - \beta_{p-1}(x_i - x)^{p-1} \right)^2
  \end{multline}
  \begin{equation}
   \hat{m}(x) = \hat{\beta}_0(x)
  \end{equation}
 \end{subequations}

Let us define the following matrices:
\begin{equation*}
 \mathcal{X}=\begin{pmatrix} 
1 & (x_1-x) & \cdots & (x_1-x)^p \\
1 & (x_2-x) & \cdots & (x_2-x)^p \\
\vdots & \vdots &    & \vdots \\
1 & (x_n-x) & \cdots & (x_n-x)^p
\end{pmatrix} 
, \mathcal{Y}=\begin{pmatrix} 
y_1\\
y_2 \\
\vdots\\
y_n
\end{pmatrix} 
\end{equation*}
and 
\begin{equation*}
 \mathcal{W}=\begin{pmatrix} 
K_h(x-x_1) & 0 & \cdots & 0\\
0 & K_h(x-x_2) & \cdots & 0 \\
\vdots & \vdots &  \ddots  & \vdots \\
0 & 0 & \cdots & K_h(x-x_n)
\end{pmatrix} 
\end{equation*}

The beta-coefficients $\boldsymbol{\beta}=(\beta_0, \dots, \beta_{p-1})$ are estimated as

\begin{equation*}
 \hat{\boldsymbol{\beta}} = \left(\mathcal{X}^T\mathcal{W}\mathcal{X}\right)^{-1}\mathcal{X}^T\mathcal{W}\mathcal{Y}
\end{equation*}
% Source: http://de.wikipedia.org/wiki/Kernel-Regression#Sch.C3.A4tzung_der_Beta-Koeffizienten

Note that the Nadaraya - Watson estimator $\hat{m}(x)$ can be computed likewise, by setting
$\mathcal{X} = \begin{pmatrix}1 & 1 & \cdots & 1\end{pmatrix}^T$
\end{theory}
 
\begin{code}
 # Important input values
 span <- 0.75 # [0,1] Portion of data that is used for each fit (bandwidth).
 enp.target <- # Targeted degree of freedom. It's an alternative way to specify span.
 degree <- 2 # The degree of the polynomials to be used.
 
 # Method
 LoessObject <- loess(y ~ x, span = span, enp.target = enp.target, degree = degree)
 
 # Important output values
 m <- fitted(LoessObject) # Same as LoessObject\$fitted
 df <- LoessObject\$trace.hat # Trace of hat matrix = degrees of freedom
\end{code}

\subsection{Smoothing splines and penalized regression}
\label{subsec:smoothing_splines}
%TODO: Why predict?
\begin{code}
 # Important input values
 df <- # degrees of freedom
 cv <- FALSE # Ordinary (TRUE) or generalized cross-validation (FALSE)

 # Method
 SplineObject <- smooth.spline(x, y, df = df, cv = cv)
 
 # Important output values
 m <- fitted(SplineObject) # Same as predict(SplineObject, x=x)\$y
 score <- SplineObject\$cv.crit # Cross-validation score
\end{code}

\subsection{Bandwidth optimization}
\begin{code}
 library(lokern)
 # Important input values
 x.out <- x # Vector of output design points
 
 # Methods
 LokernObject <- lokerns(x, y, x.out = x.out)
 GlkernObject <- glkerns(x, y, x.out = x)

 # Important output values
 m_l <- LokernObject\$est # fitted values
 m_g <- GlkernObject\$est # fitted values
 
 bw_lok <- LokernObject\$bandwidth # local bandwidth array
 bw_glk <- GlkernObject\$bandwidth # global bandwidth
\end{code}

\subsection{Hat matrix aka Smoother matrix}
\label{subsec:hatMatrix}
\begin{theory}
If the estimator is linear in $Y$, it can be viewed as a matrix-multiplication.
\begin{equation*}
 \hat{\mathbf{y}} = \mathcal{S}\mathbf{y}
\end{equation*}

The estimator can be written as follows
\begin{equation*}
 \hat{m}(x) = \sum_{i=1}^n \omega_i(x)y_i
\end{equation*}

Then, the hat matrix is defined as
\begin{equation*}
 [\mathcal{S}]_{r,s} = \omega_s(x_r)\quad r,s \in {1, ..., n}
\end{equation*}

Covariance matrix
\begin{subequations}
 \begin{gather*}
  \Cov(\hat{m}(x_i), \hat{m}(x_j)) = \sigma_\epsilon^2(\mathcal{S}\mathcal{S}^T)_{ij}\\
  \Var(\hat{m}(x_i)) = \sigma_\epsilon^2(\mathcal{S}\mathcal{S}^T)_{ii}
 \end{gather*}
\end{subequations}

Estimate for $\sigma_\epsilon^2$
\begin{equation*}
 \hat{\sigma}_\epsilon^2 = \frac{\sum_{i=1}^n (Y_i - \hat{m}(x_i))^2}{n - df}
\end{equation*}

Degrees of freedom
\begin{equation*}
 df = \tr(\mathcal{S})
\end{equation*}

\end{theory}

\begin{code}
 # Computing the hat matrix ``by hand''
 n <- nrow(datasrc)
 Id <- diag(n)
 hatMatrix <- matrix(0, n, n)
 for (j in 1:n)
   hatMatrix[, j] <- estimator(datasrc\$x, Id[,j]) # estimator could be ksmooth, loess, smooth.spline, etc. Arguments are x and y

 # Computing the hat matrix with sfsmisc
 library(sfsmisc)
 hatMatrix = hatMat(datasrc\$x, trace = FALSE,
             pred.sm = function(x,y) estimator(x, y)\$y # depending on the estimator fitted instead of y has to be used
 )
\end{code}

%TODO: Elaborate on this section
\subsection{General functions for estimators}
\begin{code}
 predict.lm
 predict.loess
 predict.smooth.spline
 predict.KernS
\end{code}
