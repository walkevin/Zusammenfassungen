\section{Bootstrap}
\subsection{Bootstrap algorithm}
\label{subsec:bootstrap_algorithm}
\begin{application}
\begin{itemize}
 \item Useful for making statistical inference (confidence intervals, mean, variance, etc.)
 \item[\leftthumbsup] Bootstrap not only valid for large $n$, as opposed to central limit theorem.
\end{itemize}
\end{application}
\begin{theory}
 Given: data set $\{Z_1, \cdots, Z_n\} \text{ i.i.d.} \sim P$, $P$ unknown. $Z_i$ may be a vector.
 \begin{enumerate}
  \item Draw $n$ uniform random samples with replacement (dt. mit Zur{\"u}cklegen) from data set to yield bootstrap sample
        \begin{equation*}
         \{Z_1^*, \cdots, Z_n^*\}
        \end{equation*} 
  \item Compute bootstrapped estimator (e.g. mean)
        \begin{equation*}
         \hat{\theta}_n^{\ast} = g(Z_1^*, \cdots, Z_n^{\ast})
        \end{equation*}
  \item Repeat steps 1 \& 2 $B$ times to get
        \begin{equation*}
         \hat{\theta}_1^{\ast}, \cdots, \hat{\theta}_n^B
        \end{equation*}
  \item Compute approximate bootstrap expectation, variance and quantiles for the estimator
        \begin{align*}
        \mathbb{E}[\hat{\theta}^{\ast}_n] & \approx \frac{1}{B} \sum_{i=1}^B \hat{\theta}_n^{\ast i}\\
        \Var^{\ast}                       & \approx \frac{1}{B-1} \sum_{i=1}^B \left( \hat{\theta}_n^{\ast i} - \mathbb{E}[\hat{\theta}^{\ast}_n] \right)^2
        \end{align*}  
        The quantiles are just the empirical quantiles of $\hat{\theta}_1^{\ast}, \cdots, \hat{\theta}_n^B$
 \end{enumerate}
\end{theory}
%TODO: insert boot package code
\begin{code}
 # Define estimator
 g <- function(ind, data){ # ind: n x 1 matrix
    bst.smp <- data[ind,] # Draw a bootstrap sample
    # Do anything with the sample, e.g. mean(bst.smp)
    # Return a p x 1 matrix
 }
 
 # Create a n x B matrix of random indices
 n <- nrow(data) # Assume one row corresponds to one datapoint Z_i
 ind <- replicate(B, sample.int(n, replace = TRUE)) # n x B matrix
 
 # Compute bootstrapped estimators (of dimension p)
 bst.est <- apply(ind, 2, g, data = data) # p x B matrix
 
 # Compute expectation, variance, quantiles
 bst.ex  <- apply(bst.est, 1, mean) # p x 1 matrix
 bst.var <- apply(bst.est, 1, var)  # p x 1 matrix
 alpha <- 0.1
 bst.qt  <- apply(bst.est, 1, quantile, probs=c(alpha/2, 1-alpha/2)) # p x 2 matrix

 # Caution! The quantiles do not coincide with the confidence interval.
 \end{code}
 \subsection{Bootstrap confidence interval}
 
 \subsection{Double bootstrap}
 
 \subsection{Bootstrap estimate of the generalization error}
 \begin{theory}
  The practical algorithm is:
  \begin{enumerate}
   \item Generate the usual bootstrap sample $(X_1^{\ast}, Y_1^{\ast}), \cdots, (X_n^{\ast}, Y_n^{\ast})$
   \item Compute a bootstrapped estimator $\hat{m}^{\ast}$
   \item Evaluate $\text{err}^{\ast} = \frac{1}{n} \sum_{i=1}^n \varrho(Y_i, \hat{m}^{\ast}(X_i))$
   \item Repeat steps 1-3 $B$ times and take the mean.
  \end{enumerate}
  \begin{itemize}
   \item This algorithm fits nicely into the general framework from \ref{subsec:bootstrap_algorithm}. Only $g$ has to be changed (see code)
   \item The \textbf{out-of-bootstrap sample} variant doesn't evaluate $\hat{m}^{\ast}$ at points, which have already been used to construct the estimator.
  \end{itemize}
 \end{theory}

 \begin{code}
  # Bootstrap estimate of generalization error
  g <- function(ind, data){# ind: m x 1 matrix, data: n x p matrix, 1.col response, other cols predictors
    bst.sample <- data[ind, ] # m x p matrix
    m <- someEstimatorFn(bst.sample)
    # Evaluate m at sample points
    mx <- predict(m, bst.sample[,-1]) # n x 1 matrix
    # Compute L1 error norm
    mean(abs(data[``y''] - mx))
  }
  
  # Out-of-bootstrap sample
  g2 <- function(ind, data){# ind: m x 1 matrix, data: n x p matrix
    m.sample <- data[ind, ] # m x p matrix
    m <- someEstimatorFn(m.sample)
    # Evaluate m at out-of-sample points
    mx <- predict(m, data[-ind,-1]) # n x 1 matrix
    # Compute L1 error norm
    mean(abs(data[``y''] - mx))
  }
 \end{code}

 \subsection{Model-based bootstrap}
 \begin{application}
 \begin{itemize}
  \item[\leftthumbsup] If parametric model is very good, then it yields more accurate variance estimates or confidence intervals.
  \item[\leftthumbsdown] Nonparametric bootstrap is less sensitive to model misspecification.
 \end{itemize}
 \end{application}
 
 \begin{theory}
  Given: a (linear) model with fixed predictors $\mathbf{x}_i \in \mathbb{R}^p$
  \begin{gather*}
      Y_i = \boldsymbol{\beta}^T \mathbf{x}_i + \epsilon_i \quad (i = 1, \cdots, n) \\
     \epsilon_1, \cdots, \epsilon_n \text{ i.i.d} \sim \mathcal{N}(0, \sigma^2), \theta = (\beta, \sigma^2)
  \end{gather*}

  The bootstrap sample is constructed as follows:
  \begin{enumerate}
   \item Estimate $\hat{\sigma}^2$ from original data.
   \item Simulate $\epsilon_1^{\ast}, \cdots, \epsilon_n^{\ast}$ i.i.d $\sim \mathcal{N}(0, \hat{\sigma}^2)$
   \item Estimate $\boldsymbol{\hat{\beta}}$ from original data.
   \item Construct
         \begin{equation*}
             Y_i^{\ast} = \boldsymbol{\hat{\beta}}^T \mathbf{x}_i + \epsilon^{\ast}_i \quad (i = 1, \cdots, n)
         \end{equation*}
         The bootstrap samples are then
         \begin{equation*}
          Z_1, \cdots, Z_n = (\mathbf{x}_1, Y_1^{\ast}, \cdots, \mathbf{x}_n, Y_n^{\ast})
         \end{equation*}
  \end{enumerate}
 \end{theory}
 
 \begin{code}
  # Given: Y, x1 (intercept), x2, x3 all n x 1 matrices
  dat <- data.frame(Y, x2, x3)
  
  empiricalSd <- sd(Y)
  eps <- rnorm(n, mean=0, sd=empiricalSd)
  beta <- coef(lm.fit(Y~x2+x3, dat))
  Y_2 <- cbind(x1, x2, x3) %*% beta
  
  # Get bootstrap samples
  bst.samples <- cbind(x2, x3, Y_2)
 \end{code}
