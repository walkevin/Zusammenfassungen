\section{Bootstrap}
\subsection{Bootstrap algorithm}
\label{subsec:bootstrap_algorithm}
\begin{application}
\begin{itemize}
 \item Useful for making statistical inference (confidence intervals, mean, variance, etc.)
 \item[\leftthumbsup] Bootstrap not only valid for large $n$, as opposed to central limit theorem.
\end{itemize}
\end{application}
\begin{theory}
 Given: data set $\{Z_1, \cdots, Z_n\} \text{ i.i.d.} \sim P$, $P$ unknown. $Z_i$ may be a vector.
 \begin{enumerate}
  \item Draw $n$ uniform random samples with replacement (dt. mit Zur{\"u}cklegen) from data set to yield bootstrap sample
        \begin{equation*}
         \{Z_1^*, \cdots, Z_n^*\}
        \end{equation*} 
  \item Compute bootstrapped estimator (e.g. mean)
        \begin{equation*}
         \hat{\theta}_n^{\ast} = g(Z_1^*, \cdots, Z_n^{\ast})
        \end{equation*}
  \item Repeat steps 1 \& 2 $B$ times to get
        \begin{equation*}
         \hat{\theta}_1^{\ast}, \cdots, \hat{\theta}_n^B
        \end{equation*}
  \item Compute approximate bootstrap expectation, variance and quantiles for the estimator
        \begin{align*}
        \mathbb{E}[\hat{\theta}^{\ast}_n] & \approx \frac{1}{B} \sum_{i=1}^B \hat{\theta}_n^{\ast i}\\
        \Var^{\ast}                       & \approx \frac{1}{B-1} \sum_{i=1}^B \left( \hat{\theta}_n^{\ast i} - \mathbb{E}[\hat{\theta}^{\ast}_n] \right)^2
        \end{align*}  
        The quantiles are just the empirical quantiles of $\hat{\theta}_1^{\ast}, \cdots, \hat{\theta}_n^B$
 \end{enumerate}
\end{theory}
%TODO: insert boot package code
\begin{code}
 # Define estimator
 g <- function(ind, data){ # ind: n x 1 matrix
    bst.smp <- data[ind,] # Draw a bootstrap sample
    # Do anything with the sample, e.g. mean(bst.smp)
    # Return a p x 1 matrix
 }
 
 # Create a n x B matrix of random indices
 n <- nrow(data) # Assume one row corresponds to one datapoint Z_i
 ind <- replicate(B, sample.int(n, replace = TRUE)) # n x B matrix
 
 # Compute bootstrapped estimators (of dimension p)
 bst.est <- apply(ind, 2, g, data = data) # p x B matrix
 
 # Compute expectation, variance, quantiles
 bst.ex  <- apply(bst.est, 1, mean) # p x 1 matrix
 bst.var <- apply(bst.est, 1, var)  # p x 1 matrix
 alpha <- 0.1
 bst.qt  <- apply(bst.est, 1, quantile, probs=c(alpha/2, 1-alpha/2)) # p x 2 matrix

 # Caution! The quantiles do not coincide with the confidence interval.
 \end{code}
 \subsection{Bootstrap confidence interval}
 \begin{theory}
  \begin{equation*}
   [2\hat{\theta}_n - q^{\ast}_{1-\alpha/2}, 2\hat{\theta}_n - q^{\ast}_{\alpha/2}]
  \end{equation*}
  where $\hat{q}_{\alpha} = q_{\alpha}^{\ast} - \hat{\theta}_n$, with $q^{\ast}_{\alpha}$ = $\alpha$-bootstrap quantile of $\hat{\theta}^{\ast}_n$  
 \end{theory}

 \subsection{Double bootstrap}
 \begin{application}
  \begin{itemize}
   \item[\leftthumbsup] Better accuracy than simple confidence interval above
  \end{itemize}

 \end{application}

 \begin{code}
  g <- function(ind, data){
    bst.smp <- data[ind,]
    # someEstimatorFn(bst.smp)
    # someResult
  }
 
  confidenceInterval <- function(ind2, data, bst.est, alpha){
    bst2.est <- apply(ind2, 2, g, data = data) #\hat{\theta}^{\ast\ast}
    bst2.qt <- apply(bst2.est - bst.est, probs=c(alpha/2, 1-alpha/2))
    c(bst.est - bst2.qt[1], bst.est - bst2.qt[2])
  }
 
  coverages <- function(ind, data, est, alpha){ # ind: n x 1 matrix
    # Estimate theta
    bst.est <- apply(ind, 2, g, data = data) #|$\hat{\theta}^{\ast}$|
    
    # Create B 2nd level bootstrap indices
    ind2 <- replicate(B, sample(ind, n, replace = TRUE)) # n x B matrix
    ci <- apply(ind2, 2, confidenceInterval, data = data, bst.est = bst.est, alpha = alpha) # 2 x B matrix
    
    # Compute coverages
    coverages <- est >= ci[1] & ci < ci[2]
  }
 
  # Compute estimate of original data
  est <- g(1:nrow(data), data)
  alpha_prime <- 0.5
  a <- 0
  b <- 1
  tol <- 1e-04
  maxIter <- 20
  i <- 0
  converged <- FALSE
  while(i < maxIter){ # Search for right alpha_prime with bisection
    alpha_prime <- (a+b)/2
    # Get first level bootstrap random indices
    ind <- replicate(M, sample.int(n, replace = TRUE)) # n x M matrix
    # Get M bootstrapped estimates of coverage
    cover <- apply(ind, 2, coverages, data = data, est = est, alpha = alpha_prime)
    # Probability that est is in confidence interval
    pa <- mean(cover)
    
    if(abs(pa - alpha) < tol){
        converged <- TRUE
        break;
    }
    else{
        if(pa < alpha){
            a <- a+(b-a)/2
        }
        else{
            b <- b-(b-a)/2
        }
    }
    i <- i+1
  }
  
  if(!converged){
    cat("Not converged.")
  }
  
  # Return corrected confidence interval level
  alpha_prime
 \end{code}

 \subsection{Bootstrap estimate of the generalization error}
 \begin{theory}
  The practical algorithm is:
  \begin{enumerate}
   \item Generate the usual bootstrap sample $(X_1^{\ast}, Y_1^{\ast}), \cdots, (X_n^{\ast}, Y_n^{\ast})$
   \item Compute a bootstrapped estimator $\hat{m}^{\ast}$
   \item Evaluate $\text{err}^{\ast} = \frac{1}{n} \sum_{i=1}^n \varrho(Y_i, \hat{m}^{\ast}(X_i))$
   \item Repeat steps 1-3 $B$ times and take the mean.
  \end{enumerate}
  \begin{itemize}
   \item This algorithm fits nicely into the general framework from \ref{subsec:bootstrap_algorithm}. Only $g$ has to be changed (see code)
   \item The \textbf{out-of-bootstrap sample} variant doesn't evaluate $\hat{m}^{\ast}$ at points, which have already been used to construct the estimator.
  \end{itemize}
 \end{theory}

 \begin{code}
  # Bootstrap estimate of generalization error
  g <- function(ind, data){# ind: m x 1 matrix, data: n x p matrix, 1.col response, other cols predictors
    bst.sample <- data[ind, ] # m x p matrix
    m <- someEstimatorFn(bst.sample)
    # Evaluate m at sample points
    mx <- predict(m, bst.sample[,-1]) # n x 1 matrix
    # Compute L1 error norm
    mean(abs(data["y"] - mx))
  }
  
  # Out-of-bootstrap sample
  g2 <- function(ind, data){# ind: m x 1 matrix, data: n x p matrix
    m.sample <- data[ind, ] # m x p matrix
    m <- someEstimatorFn(m.sample)
    # Evaluate m at out-of-sample points
    mx <- predict(m, data[-ind,-1]) # n x 1 matrix
    # Compute L1 error norm
    mean(abs(data["y"] - mx))
  }
 \end{code}

 \subsection{Model-based bootstrap}
 \begin{application}
 \begin{itemize}
  \item[\leftthumbsup] If parametric model is very good, then it yields more accurate variance estimates or confidence intervals.
  \item[\leftthumbsdown] Nonparametric bootstrap is less sensitive to model misspecification.
 \end{itemize}
 \end{application}
 
 \begin{theory}
  Given: a (linear) model with fixed predictors $\mathbf{x}_i \in \mathbb{R}^p$
  \begin{gather*}
      Y_i = \boldsymbol{\beta}^T \mathbf{x}_i + \epsilon_i \quad (i = 1, \cdots, n) \\
     \epsilon_1, \cdots, \epsilon_n \text{ i.i.d} \sim \mathcal{N}(0, \sigma^2), \theta = (\beta, \sigma^2)
  \end{gather*}

  The bootstrap sample is constructed as follows:
  \begin{enumerate}
   \item Estimate $\hat{\sigma}^2$ from original data.
   \item Simulate $\epsilon_1^{\ast}, \cdots, \epsilon_n^{\ast}$ i.i.d $\sim \mathcal{N}(0, \hat{\sigma}^2)$
   \item Estimate $\boldsymbol{\hat{\beta}}$ from original data.
   \item Construct
         \begin{equation*}
             Y_i^{\ast} = \boldsymbol{\hat{\beta}}^T \mathbf{x}_i + \epsilon^{\ast}_i \quad (i = 1, \cdots, n)
         \end{equation*}
         The bootstrap samples are then
         \begin{equation*}
          Z_1, \cdots, Z_n = (\mathbf{x}_1, Y_1^{\ast}, \cdots, \mathbf{x}_n, Y_n^{\ast})
         \end{equation*}
  \end{enumerate}
 \end{theory}
 
 \begin{code}
  # Given: Y, x1 (intercept), x2, x3 all n x 1 matrices
  dat <- data.frame(Y, x2, x3)
  
  empiricalSd <- sd(Y)
  eps <- rnorm(n, mean=0, sd=empiricalSd)
  beta <- coef(lm.fit(Y~x2+x3, dat))
  Y_2 <- cbind(x1, x2, x3) %*% beta
  
  # Get bootstrap samples
  bst.samples <- cbind(x2, x3, Y_2)
 \end{code}
