\section{Classification}
\begin{theory}
 \begin{itemize}
  \item Response variable $Y$ is element of a discrete set $\{0, \cdots, J-1\}$
  \item The discrete version of an estimator $\hat{m}$ is the probability function
        \begin{equation*}
         \pi_j(x) = \mathbb{P}[Y=j|X=x]\quad(j=0,1,\cdots,J-1)
        \end{equation*}
  \item A \textbf{classifier} is the function
        \begin{equation*}
         \mathcal{C}: \mathbb{R}^p \rightarrow \{0, \cdots, J-1\}
        \end{equation*}
  \item The \textbf{Bayes classifier} is
        \begin{equation*}
         \mathcal{C}_{\text{Bayes}}(x) = \argmax_{0\leq j<J-1} \pi_j(x)
        \end{equation*}
  \item A discrete version of the loss function is
        \begin{equation*}
         \varrho(x, x') = \begin{cases}
                           0 & \text{if }x = x'\\
                           1 & \text{else}
                          \end{cases}
        \end{equation*}
  \end{itemize}
\end{theory}
\subsection{Linear discriminant analysis (LDA)}
\begin{theory}
 LDA is based on the following model
 \begin{align*}
  (X|Y=j) & \sim \mathcal{N}_p(\mu_j, \boldsymbol{\Sigma}),\\
  P[Y=j]  & = p_j, \quad \sum_j^{J-1}p_j=1
 \end{align*}
 \begin{itemize}
  \item Especially, the covariance $\boldsymbol{\Sigma}$ is the same for each group.
  \item Decision boundaries (boundaries between different classes) are hyperplanes.
 \end{itemize} 
\end{theory}
\begin{code}
 data <- iris[,c("Petal.Length","Petal.Width")] # A data set
 classes <- as.integer(Iris[,"Species"]) # These are the classes
 library(MASS)
 # Train the LDA estimator
 class_lda <- lda(x=data[,c("Petal.Length", "Petal.Width")], grouping=classes)

 # Use it to predict
 n <- 10
 xp <- seq(min(data[,1]), max(data[,1]), length = n)
 yp <- seq(min(data[,2]), max(data[,2]), length = n)
 grid <- cbind(rep(xp, n), rep(yp, each=n))
 
 z <- predict(class_lda, data.frame(grid))
 
 # Plot classes
 plot(grid[,1], grid[,2], col=as.numeric(z\$class), pch=as.numeric(z\$class))
 
 # Plot contours (Use z\$post values for smoother contours)
 nLevels = length(unique(classes))
 contour(grid[,1], grid[,2], as.numeric(z\$class), levels=1:nLevels, add=TRUE)
 
 # Overoptimistic error estimate
 fit_lda <- predict(class_lda)
 error_lda <- mean(as.numeric(classes) != fit_lda)
\end{code}


\subsection{Quadratic discriminant analysis (QDA)}
\begin{theory}
 QDA is based on the following model
 \begin{align*}
  (X|Y=j) & \sim \mathcal{N}_p(\mu_j, \boldsymbol{\Sigma}_j),\\
  P[Y=j]  & = p_j, \quad \sum_j^{J-1}p_j=1
 \end{align*}
 \begin{itemize}
  \item Covariance $\boldsymbol{\sigma}$ varies for each group.
  \item[\leftthumbsup] More flexible and general than LDA
  \item[\leftthumbsdown] Typically overfits for large $p$ (dimension)
 \end{itemize} 
\end{theory}
\begin{code}
 # See LDA Example
 library(MASS)
 qda(x, grouping, ...)
\end{code}

\subsection{Logistic regression}
\subsubsection{Binary classification}
\begin{theory}
 \begin{itemize}
  \item Logistic model for $\pi(\cdot)$
        \begin{align*}
                         & \log\left( \frac{\pi(x)}{1-\pi(x)} \right) = g(x)\\
         \Leftrightarrow & \pi(x) = \frac{e^{g(x)}}{1+e^{g(x)}}\\
                         & g: \mathbb{R}^p \rightarrow \mathbb{R}
        \end{align*}
  \item A simple model for $g(\cdot)$ is
        \begin{equation*}
         g(x) = \sum_{j=1}^p \beta_jx_j
        \end{equation*}
  \item The parameters $\beta_j$ can be fitted using maximum-likelihood.
  \item Recap: Max-likelihood
        \begin{gather*}
         Y_1, \cdots, Y_n \text{ i.i.d., } Y_i \sim f_{\pi, \cdots}(Y_i)\\
         L(\pi, \cdots; \vec{Y}) = \prod_{i=1}^n f_{\pi, \cdots}(Y_i)\\
         \ell := \log(L) = \sum_{i=1}^n \log f_{\pi, \cdots}(Y_i)
        \end{gather*}\\
        Example: $Y_i \sim \text{Bernoulli}(\pi(x_i))$. Then, $f = f_{\pi(x_i)}(Y_i) = \pi(x_i)^{Y_i} (1-\pi(x_i))^{1-Y_i}$
 \end{itemize}
\end{theory}

\begin{code}
 ## Manual max likelihood
 neg.logL <- function(beta, dat){
  g <- beta[1] + beta[2] * dat\$x;
  -sum(g - log(1 + exp(g)))
 }
 
 # Compute minimizer
 opt.beta <- optim(c(0,0), neg.logL, dat = someData)
 
 # Predict some new values
 new.X <- 10:100
 g2 <- opt.beta[1] + opt.beta[2] * new.X
 new.Y <- exp(g2) / (1+exp(g2))
 
 ## Do the same using glm()
 fit <- glm(Y ~ X, data = someData, family=``binomial'')
 new.Y_glm <- predict(fit, newdata=data.frame(X=new.X))
 
 # See family in R-help for details about binomial etc.
 |%# glm(formula = cbind(N, m-N) ~ age , family=``binomial'') for Bin(n,p) distribution|
\end{code}


